{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b305e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# import string\n",
    "\n",
    "# import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chat = pd.read_csv('prod_snapshot/firestore-chatHistory-prod.csv')\n",
    "df_chat.head()\n",
    "# df_chat.shape\n",
    "# df_chat['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89726940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query = pd.read_csv('prod_snapshot/firestore-userQuery-prod.csv')\n",
    "df_query.head(1)\n",
    "# df_query.shape\n",
    "# df_query['id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30773d09",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e34a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate queries within a certain period (10 minutes) to prevent spam entries from interfering with analysis \n",
    "\n",
    "# Convert query_timestamp to datetime\n",
    "df_query['query_timestamp'] = pd.to_datetime(\n",
    "    df_query['query_timestamp'], \n",
    "    format=\"%a, %d %b %Y %H:%M:%S GMT\",  # matches \"Tue, 22 Jul 2025 15:31:56 GMT\"\n",
    "    utc=True\n",
    ")\n",
    "\n",
    "# Keep only the first query in each 10-minute window per unique query_text\n",
    "df_query = df_query.groupby('query_text').apply(\n",
    "    lambda g: g[g['query_timestamp'].diff().dt.total_seconds().div(60).fillna(999) > 10]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839dbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying (& removing) non-English queries\n",
    "\n",
    "def replace_smart_chars(text):\n",
    "    \"\"\"\n",
    "    Replace common non-ASCII quotes/apostrophes/dashes/ellipsis with ASCII.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    replacements = {\n",
    "        '‚Äô': \"'\",   # right single quote\n",
    "        '‚Äò': \"'\",   # left single quote\n",
    "        '‚Äú': '\"',   # left double quote\n",
    "        '‚Äù': '\"',   # right double quote\n",
    "        '‚Äì': '-',   # en dash\n",
    "        '‚Äî': '-',   # em dash\n",
    "        '‚Ä¶': '...', # ellipsis\n",
    "    }\n",
    "\n",
    "    for orig, repl in replacements.items():\n",
    "        text = text.replace(orig, repl)\n",
    "\n",
    "    return text\n",
    "\n",
    "def contains_non_english(text):\n",
    "    \"\"\"\n",
    "    Returns True if the text contains non-English characters,\n",
    "    ignoring digits, punctuation, and spaces.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "\n",
    "    # Replace smart chars first\n",
    "    text = replace_smart_chars(text)\n",
    "\n",
    "    # Remove digits, punctuation, spaces\n",
    "    text_clean = re.sub(r'[\\d\\s!\"#$%&\\'()*+,-./:;<=>?@[\\\\\\]^_`{|}~]', '', text)\n",
    "\n",
    "    # If any character is non-ASCII, return True\n",
    "    return bool(re.search(r'[^\\x00-\\x7F]', text_clean))\n",
    "\n",
    "df_query = df_query[~df_query['query_text'].apply(contains_non_english)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a90849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant queries\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "# nltk.download('words')\n",
    "\n",
    "word_list = set(words.words())\n",
    "irrelevant_phrases = {\"hello\", \"hello there\", \"hi\", \"hi there\", \"help\", \"help me\", \"test\", \"testing\", \"care corner\", \"carecorner\", \"schemes sg\", \"schemes\", \"sg\", \n",
    "                      \"thank you\", \"thanks\", \"bye\", \"goodbye\", \"good bye\", \"toa payoh\"}\n",
    "\n",
    "def is_relevant_query(query):\n",
    "    \"\"\"\n",
    "    Returns True if the query is relevant, False otherwise.\n",
    "    Filters out:\n",
    "      - Single letters\n",
    "      - Only numbers\n",
    "      - Only punctuation\n",
    "      - Empty strings\n",
    "      - Specific irrelevant phrases\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(query, str) or query.strip() == \"\":\n",
    "        return False\n",
    "\n",
    "    # # Remove leading/trailing punctuation\n",
    "    # query_clean = re.sub(r'^\\W+|\\W+$', '', query)\n",
    "\n",
    "    # query_clean = re.sub(r\"[^\\w\\s]\", \" \", query).strip().lower()  # lowercase, remove punctuations (replace with space)\n",
    "\n",
    "    # query_clean = query.encode('ascii', 'ignore').decode()       # strip fancy punctuation\n",
    "    # query_clean = re.sub(r\"[^\\w\\s]\", \" \", query_clean, flags=re.UNICODE)\n",
    "    # query_clean = re.sub(r\"\\s+\", \" \", query_clean).strip().lower()\n",
    "    \n",
    "    # 1. Single character\n",
    "    if len(query) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # 2. Only numbers\n",
    "    if query.isdigit():\n",
    "        return False\n",
    "    \n",
    "    # 3. Only punctuation / symbols\n",
    "    if not re.search(r'\\w', query):\n",
    "        return False\n",
    "    \n",
    "    # 4. Remove queries in the irrelevant phrases list\n",
    "    if query in irrelevant_phrases:\n",
    "        return False\n",
    "    \n",
    "    # Check tokens\n",
    "    tokens = re.findall(r\"[a-zA-Z]+\", query)\n",
    "\n",
    "    # Require at least one real word from dictionary\n",
    "    if not any(t in word_list for t in tokens):\n",
    "        return False\n",
    "    \n",
    "    # Otherwise, keep\n",
    "    return True\n",
    "\n",
    "df_clean = df_query[df_query['query_text'].apply(is_relevant_query)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1059c49f",
   "metadata": {},
   "source": [
    "Identifying structured queries that follow the \"I am ... looking for ...\" format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of pre-defined categories on the portal\n",
    "# I am {user_profiles} looking for {scheme_type} that offers {support}\n",
    "\n",
    "user_profiles = [\"Low-Income Individual or Family\", \"Parent or Family Member\", \"Elderly (Senior)\", \"Person with Disabilities or Special Needs\", \"Caregiver\", \"Child or Youth\", \n",
    "                 \"Ex-Offender or Incarcerated Individual\", \"Migrant or Foreign Worker\", \"Woman in Need of Support\", \"Facing Mental Health Challenges\", \n",
    "                 \"Homeless or in Need of Shelter\", \"Dealing with Addictions or Recovery\", \"Facing End-of-Life or Terminal Illness\", \"In Need of Legal Aid\", \n",
    "                 \"Experiencing Abuse or Violence\"]\n",
    "\n",
    "scheme_type = [\"Financial Assistance Programs\", \"Food Support\", \"Housing Assistance\", \"Shelter Services\", \"Healthcare Services\", \"Mental Health Support\", \"Education Programs\", \n",
    "               \"Employment Assistance\", \"Caregiver Support\", \"Transport Services\", \"Legal Aid Services\", \"Addiction Recovery Services\", \"Family Support Services\", \n",
    "               \"Disability Support Services\", \"Palliative Care Services\", \"Social Work & Casework\"]\n",
    "\n",
    "support = [\"Financial Assistance\", \"Food Support\", \"Housing Assistance\", \"Healthcare Services\", \"Mental Health Support\", \"Education Opportunities\", \"Employment Support\", \n",
    "           \"Caregiver Assistance\", \"Transport Mobility Support\", \"Legal Aid Services\", \"Addiction Recovery Services\", \"Parenting Support\", \"Disability Support\", \n",
    "           \"Palliative Care Services\", \"Social Work Services\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc13d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep punctuations intact for now (to match with keywords that have punctuations in them)\n",
    "\n",
    "def clean_text(text):\n",
    "    # text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)  # keep only letters & spaces\n",
    "    text = re.sub(r\"[^a-zA-Z\\s.,!?&'\\-()]\", \" \", text) # Keep all common punctuation\n",
    "    return text\n",
    "\n",
    "# df_profile = df_clean.loc[df_clean['query_text'].str.contains(r\"\\bi am\\b.*\\blooking for\\b.*\\bthat offers\\b\", case=False, na=False)].copy()\n",
    "df_profile = df_clean.loc[df_clean['query_text'].str.contains(r\"\\bi am\\b.*\\blooking for\\b.\", case=False, na=False)].copy()\n",
    "\n",
    "# astype(str) converts NaN and floats into the string \"nan\", which is safe to search.\n",
    "# ‚úÖ na=False ensures missing values are treated as non-matches, not errors.\n",
    "df_profile.loc[:, 'clean_query'] = df_profile['query_text'].astype(str).apply(clean_text)\n",
    "\n",
    "# pattern = re.compile(r\"\\bi am\\b\\s*(.*?)\\s*\\blooking for\\b\\s*(.*?)\\s*\\bthat offers\\b\\s*(.*)\", re.IGNORECASE)\n",
    "pattern = re.compile(r\"\\bi\\s*am(?:\\s+(.*?))?\\s+looking for\\s+(.*)\", re.IGNORECASE)\n",
    "\n",
    "# def extract_profile(query):\n",
    "#     match = pattern.search(query)\n",
    "#     if match:\n",
    "#         who = match.group(1).strip() if match.group(1) else \"\"   # blank if not provided\n",
    "#         what = match.group(2).strip()\n",
    "#         return who, what\n",
    "#     return None, None\n",
    "\n",
    "df_profile[['who_am_i', 'looking_for']] = df_profile['clean_query'].fillna(\"\").str.extract(pattern).astype(str)\n",
    "# df_profile.loc[:, ['who_am_i', 'looking_for']] = df_profile['clean_query'].apply(lambda x: pd.Series(extract_profile(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out queries for user profiles that do not have any matches in the respective list:\n",
    "\n",
    "# df_profile_filtered = df_profile[~((df_profile['who_am_i'].isin(user_profiles)) | (df_profile['who_am_i'].str.strip() == \"\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a90a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_profile[df_profile['id'] == '068a5baa-39d9-11f0-89bf-42004e494300']['query_text'].squeeze())\n",
    "# print(df_profile[df_profile['id'] == '0cb6f3d6-f99b-11ef-9f34-42004e494300']['query_text'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff847d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual corrections for specific entries\n",
    "\n",
    "# df_profile.loc[df_profile['id'] == '068a5baa-39d9-11f0-89bf-42004e494300', 'who_am_i'] = [\"Homeless or in Need of Shelter\"]\n",
    "# # df_profile.loc[df_profile['id'] == '0cb6f3d6-f99b-11ef-9f34-42004e494300', 'who_am_i'] = [[\"Person with Disabilities or Special Needs\", \"Child or Youth\"]]\n",
    "\n",
    "# # Above does not work. .at bypasses pandas‚Äô broadcasting logic, guarantees the list is stored as a list, not coerced to NaN\n",
    "\n",
    "# # Step 1: find the index of the row\n",
    "# idx = df_profile.index[df_profile['id'] == '0cb6f3d6-f99b-11ef-9f34-42004e494300'][0]\n",
    "\n",
    "# # Step 2: assign the list\n",
    "# df_profile.at[idx, 'who_am_i'] = [\"Person with Disabilities or Special Needs\", \"Child or Youth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cb166e",
   "metadata": {},
   "source": [
    "Identifying narrative/unstructured queries that do not follow the \"I am ... looking for ...\" format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef2b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations for easier manual matching of words\n",
    "\n",
    "df_clean['query_text'] = (\n",
    "    df_clean['query_text']\n",
    "    .str.replace(r\"[^\\w\\s]\", \" \", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "df_untag = df_clean.loc[~df_clean['query_text'].str.contains(r\"\\bi am\\b.*\\blooking for\\b\", case=False, na=False)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2aff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1\n",
    "\n",
    "# df_untag['clean_test'] = df_untag['query_text'].apply(\n",
    "#     lambda x: re.sub(r\"[^\\w\\s]\", \" \", str(x)).strip().lower()\n",
    "# )\n",
    "# print(df_untag['clean_test'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2061fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2\n",
    "\n",
    "# (df_untag['query_text'].str.contains(r\"[^\\w\\s]\", regex=True)).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb00e420",
   "metadata": {},
   "source": [
    "Identifying common keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da19be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load English model (run once)\n",
    "# # python -m spacy download en_core_web_sm\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # # Example DataFrame\n",
    "# # df_free = pd.DataFrame({\n",
    "# #     'query_text': [\n",
    "# #         \"I'm trying to quit drinking but it's really hard on my own.\",\n",
    "# #         \"I‚Äôm 38 and just started caring for my wife full-time.\",\n",
    "# #         \"Looking for caregiver support after surgery.\",\n",
    "# #         \"I want to find a group to talk about anxiety and stress.\"\n",
    "# #     ]\n",
    "# # })\n",
    "\n",
    "# def extract_nouns_verbs(text):\n",
    "#     \"\"\"\n",
    "#     Extract only nouns and verbs from text using spaCy.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(text, str):\n",
    "#         return []\n",
    "\n",
    "#     doc = nlp(text.lower())\n",
    "#     words = [token.lemma_ for token in doc \n",
    "#              if (token.pos_ in [\"NOUN\", \"VERB\"]) and not token.is_stop and token.is_alpha]\n",
    "#     return words\n",
    "\n",
    "# # Collect all words\n",
    "# all_words = []\n",
    "# df_untag['query_text'].dropna().apply(lambda x: all_words.extend(extract_nouns_verbs(x)))\n",
    "\n",
    "# # Generate word cloud\n",
    "# wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(\" \".join(all_words))\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(10,5))\n",
    "# plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure you have NLTK data installed once\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "# nltk.download('stopwords') \n",
    "\n",
    "all_words = []\n",
    "phrases_to_exclude = [\"social work\", \"toa payoh\", \"care corner\", \"years old\", \"years-old\", \"year-old\", \"year old\"]\n",
    "\n",
    "# custom_stopwords = set(STOPWORDS)\n",
    "custom_stopwords = set(stopwords.words(\"english\"))\n",
    "custom_stopwords.update([\"month\", \"year\", \"years\", \"available\", \"support\", \"supporting\", \"provide\", \"provides\", \"providing\", \"deal\", \"deals\", \"dealing\", \"resources\", \"service\", \"services\", \"programs\", \"program\", \"programme\", \"programmes\", \n",
    "                         \"issue\", \"issues\", \"problem\", \"problems\", \"challenges\", \"challenge\", \"challenging\", \"require\", \"requiring\", \"need\", \"needs\", \"want\", \"looking\", \"for\", \"assistance\", \"assist\", \"aid\", \"get\",\n",
    "                         \"help\", \"helping\", \"test\", \"prompt\", \"hi\", \"hello\", \"schemes\", \"scheme\", \"offers\", \"offer\", \"offering\", \"find\", \"know\", \"worried\", \"worry\", \"suffering\", \"suffer\", \"struggling\", \"struggle\", \n",
    "                         \"experiencing\", \"experience\", \"facing\", \"face\", \"apply\", \"applying\", \"singapore\", \"centre\", \"client\", \"person\", \"member\", \"casework\", \"carecorner\", \"special\", \"individual\", \"time\"]) \n",
    "\n",
    "def clean_text(text):\n",
    "    for phrase in phrases_to_exclude:\n",
    "        text = text.lower().replace(phrase, \"\")\n",
    "    return text\n",
    "\n",
    "def extract_nouns_verbs_adj(text):\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    cleaned = clean_text(text)\n",
    "\n",
    "    tokens = word_tokenize(cleaned.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # keep only words\n",
    "    # tokens = [t for t in tokens if t.isalnum()]  # keep words and numbers\n",
    "    tagged = pos_tag(tokens)\n",
    "\n",
    "    # Keep nouns (NN, NNS, NNP, NNPS) and verbs (VB, VBD, VBG, VBN, VBP, VBZ)\n",
    "    # words = [word for word, pos in tagged if pos.startswith(\"NN\") or pos.startswith(\"VB\")]\n",
    "    words = [word for word, pos in tagged if pos.startswith((\"NN\", \"VB\", \"JJ\"))]\n",
    "\n",
    "    # Remove stopwords\n",
    "    # stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if w not in custom_stopwords]\n",
    "\n",
    "    return words\n",
    "\n",
    "def unique_words_per_query(text):\n",
    "    words = extract_nouns_verbs_adj(text)\n",
    "    return list(set(words))\n",
    "\n",
    "df_clean['query_text'].dropna().apply(lambda x: all_words.extend(unique_words_per_query(x)))\n",
    "\n",
    "# filtered_words = [w for w in all_words if w.lower() not in custom_stopwords]\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\", stopwords=custom_stopwords).generate(\" \".join(all_words))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# df_word_cloud = pd.DataFrame(all_words, columns=['word'])\n",
    "# unique_words = df_word_cloud['word'].unique()\n",
    "\n",
    "# df_unique_words = pd.DataFrame(unique_words, columns=['word'])\n",
    "# df_unique_words.to_csv('word_cloud.csv', index=False)\n",
    "\n",
    "# pd.DataFrame({'word': list(set(all_words))}).to_csv('word_cloud.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec4e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words in queries\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "top_n = 20 \n",
    "common_words = word_counts.most_common(top_n)\n",
    "\n",
    "words, counts = zip(*common_words)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(words, counts, color=\"skyblue\")\n",
    "plt.xlabel(\"Count of Queries Mentioning Word\")\n",
    "plt.title(f\"Top {top_n} Most Common Words in Queries\")\n",
    "plt.gca().invert_yaxis()  # largest at top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2226218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# You already have: all_words = [] and df_clean with 'query_text'\n",
    "\n",
    "# --- Step 1. Create a list of word sets per query ---\n",
    "word_sets = []\n",
    "for text in df_clean['query_text'].dropna():\n",
    "    word_list = unique_words_per_query(text)\n",
    "    if word_list:\n",
    "        word_sets.append(set(word_list))\n",
    "\n",
    "# --- Step 2. Build co-occurrence pairs ---\n",
    "cooccur = Counter()\n",
    "for words in word_sets:\n",
    "    for w1, w2 in combinations(sorted(words), 2):  # sorted ensures ('a','b') == ('b','a')\n",
    "        cooccur[(w1, w2)] += 1\n",
    "\n",
    "# --- Step 3. Convert to DataFrame for easy analysis ---\n",
    "df_cooccur = (\n",
    "    pd.DataFrame([(a, b, c) for (a, b), c in cooccur.items()],\n",
    "                 columns=['word1', 'word2', 'count'])\n",
    "    .sort_values('count', ascending=False)\n",
    ")\n",
    "\n",
    "# Optional: Filter out weak co-occurrences (e.g. < 5)\n",
    "df_cooccur = df_cooccur[df_cooccur['count'] >= 5].reset_index(drop=True)\n",
    "\n",
    "print(df_cooccur.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883bd4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges with weight = co-occurrence count\n",
    "for _, row in df_cooccur.iterrows():\n",
    "    G.add_edge(row['word1'], row['word2'], weight=row['count'])\n",
    "\n",
    "# Keep only the top 50 edges for clarity\n",
    "edges_to_plot = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)[:50]\n",
    "H = nx.Graph()\n",
    "H.add_edges_from([(a, b, d) for a, b, d in edges_to_plot])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "pos = nx.spring_layout(H, k=0.5, seed=42)\n",
    "nx.draw(H, pos, with_labels=True, node_color='lightblue', node_size=1200, font_size=10, width=1)\n",
    "plt.title(\"Co-occurring Need Clusters in User Queries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529c06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "# --- 1Ô∏è‚É£ Identify the largest community in your existing graph ---\n",
    "communities = nx_comm.greedy_modularity_communities(H)\n",
    "# for i, c in enumerate(communities, 1):\n",
    "#     print(f\"Cluster {i}: {', '.join(sorted(c))}\")\n",
    "largest_cluster_nodes = max(communities, key=len)\n",
    "print(f\"Largest community size: {len(largest_cluster_nodes)} nodes\")\n",
    "\n",
    "# --- 2Ô∏è‚É£ Create a subgraph for just that cluster ---\n",
    "G_sub = H.subgraph(largest_cluster_nodes).copy()\n",
    "\n",
    "# --- 3Ô∏è‚É£ Run sub-community detection inside this subgraph ---\n",
    "sub_communities = nx_comm.greedy_modularity_communities(G_sub)\n",
    "print(f\"Identified {len(sub_communities)} sub-clusters within the largest cluster\")\n",
    "\n",
    "# --- 4Ô∏è‚É£ Assign colours for visual clarity ---\n",
    "color_map = {}\n",
    "palette = plt.cm.tab10.colors  # You can switch to plt.cm.Set3 for softer colours\n",
    "for i, c in enumerate(sub_communities):\n",
    "    for node in c:\n",
    "        color_map[node] = palette[i % len(palette)]\n",
    "\n",
    "# --- 5Ô∏è‚É£ Visualise the sub-clusters ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "pos = nx.spring_layout(G_sub, k=0.7, seed=42)\n",
    "\n",
    "nx.draw_networkx_nodes(\n",
    "    G_sub, pos,\n",
    "    node_color=[color_map.get(node, \"lightgrey\") for node in G_sub.nodes()],\n",
    "    node_size=1300,\n",
    "    alpha=0.9\n",
    ")\n",
    "nx.draw_networkx_edges(G_sub, pos, alpha=0.3, width=2)\n",
    "nx.draw_networkx_labels(G_sub, pos, font_size=10)\n",
    "plt.title(\"Sub-Clusters within the Largest Co-occurrence Community\", fontsize=13)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# --- 6Ô∏è‚É£ Print out sub-cluster words for interpretation ---\n",
    "for i, c in enumerate(sub_communities, 1):\n",
    "    print(f\"\\nüü© Sub-cluster {i} ({len(c)} words):\")\n",
    "    print(\", \".join(sorted(c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8fc7c2",
   "metadata": {},
   "source": [
    "Classifying user profiles and schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d093b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# --- Predefined Categories ---\n",
    "SCHEME_CATEGORIES = {\n",
    "    \"Financial Assistance Programs\": [\"financial\", \"financials\", \"financially\", \"finances\", \"finance\", \"earning\", \"earnings\", \"grant\", \"grants\", \"poor\", \"poverty\", \"cost\", \"costs\", \"fees\", \n",
    "                                      \"subsidy\", \"subsidies\", \"money\", \"bills\", \"borrow\", \"pay\", \"paying\", \"payment\", \"savings\", \"rebates\", \"rebate\", \"grants\", \"grant\", \"loans\", \"loan\", \n",
    "                                      \"debts\", \"debt\", \"comcare\", \"workfare\", \"bankrupt\", \"fines\", \"arrears\", \"vouchers\", \"voucher\", \"afford\", \"affordable\", \"budget\", \"budgeting\", \n",
    "                                      \"chas\", \"scholarship\", \"scholarships\", \"bursary\", \"bursaries\", \"bonus\", \"bonuses\", \"payout\", \"payouts\", \"fund\", \"funds\", \"funding\", \"cdc\", \"cpf\",\n",
    "                                      \"medisave\", \"medifund\", \"medishield\", \"silver support\", \"skillsfuture\", \"insurance\", \"expenses\"],\n",
    "    \"Food Support\": [\"food\", \"meal\", \"meals\", \"groceries\", \"milk\"],\n",
    "    \"Housing Assistance\": [\"housing\", \"house\", \"ownership\", \"evicted\", \"evict\", \"eviction\", \"rent\", \"rental\", \"flat\", \"hostel\", \"property\", \"landlord\", \"lease\", \"accommodation\", \n",
    "                           \"temporary housing\", \"hoard\", \"hoarding\", \"declutter\", \"decluttering\", \"hdb\", \"bto\"],\n",
    "    \"Shelter Services\": [\"shelter\", \"shelters\", \"homeless\", \"rough sleeping\", \"rough sleeper\"],\n",
    "    \"Healthcare Services\": [\"illness\", \"ill\", \"sick\", \"doctor\", \"nurse\", \"patient\", \"tcm\", \"hospital\", \"treatment\", \"healthcare\", \"medical\", \"medicine\", \"medication\", \"clinic\", \n",
    "                            \"appointment\", \"appointments\", \"health screening\"],\n",
    "    \"Mental Health Support\": [\"therapy\", \"therapist\", \"mental\", \"depression\", \"depressed\", \"anxieties\", \"anxiety\", \"anxious\", \"suicidal\", \"suicide\"],\n",
    "    \"Education Programs\": [\"school\", \"tuition\", \"education\", \"poly\", \"uni\", \"university\", \"academic\"],\n",
    "    \"Employment Assistance\": [\"job\", \"jobs\", \"jobless\", \"work\", \"layoff\", \"skills\", \"upskill\", \"upskilling\", \"development\", \"part time\", \"employment\", \"career\", \"training\", \"retrenched\", \"retrenchment\", \n",
    "                              \"layoff\", \"fired\", \"unemployed\", \"workfare\", \"gig\", \"freelance\"],\n",
    "    \"Caregiver Support\": [\"caregiver\", \"caregiving\", \"take care\", \"respite\", \"helper\", \"homebound\", \"bedridden\"],\n",
    "    \"Transport Services\": [\"transport\", \"bus\", \"fare\", \"shuttle\", \"get around\"],\n",
    "    \"Legal Aid Services\": [\"legal\", \"lawyer\", \"court\", \"sue\"],\n",
    "    \"Addiction Recovery Services\": [\"alcohol\", \"drinking\", \"drug\", \"drugs\", \"addiction\", \"rehab\", \"kpod\", \"halfway\"],\n",
    "    \"Family Support Services\": [\"family support\", \"family service\", \"childcare\", \"child care\", \"daycare\", \"home care\", \"home care\", \"orphanage\", \"custody\", \"divorce\", \"divorced\", \"marriage\", \"married\",\n",
    "                                \"nursing\", \"elder care\", \"eldercare\", \"old aged home\", \"old folks home\", \"dementia\", \"adoption\", \"foster\", \"fostering\", \"parenting\", \"baby\", \"active ageing\", \"active aging\"],\n",
    "    \"Disability Support Services\": [\"disability\", \"special needs\", \"wheelchair\"],\n",
    "    \"Palliative Care Services\": [\"end of life\", \"terminal\", \"terminally ill\", \"final stage\", \"advanced stage\", \"palliative\", \"hospice\", \"cancer\", \"dying\"],\n",
    "    \"Social Work & Casework\": [\"social worker\", \"social work\", \"counselling\", \"counsellor\", \"behavioural\", \"risk\", \"anger\", \"abuse\", \"harrassment\", \"violence\", \"sexual\", \"rape\", \"boys home\", \n",
    "                               \"girls home\", \"emotional\", \"emotions\", \"talk to\", \"trauma\", \"mediate\", \"mediation\", \"psychologist\", \"therapist\", \"therapy\", \"relationships\", \"relationship\"],\n",
    "}\n",
    "\n",
    "USER_PROFILES = {\n",
    "    \"Low-Income Individual or Family\": [\"income\", \"budget\", \"finances\", \"financials\", \"financial\", \"financially\", \"poor\", \"afford\", \"underprivileged\"],\n",
    "    \"Parent or Family Member\": [\"parent\", \"parents\", \"mother\", \"mum\", \"father\", \"dad\", \"family member\", \"divorce\", \"divorced\", \"baby\"],\n",
    "    \"Elderly (Senior)\": [\"elderly\", \"elders\", \"elder\", \"senior\", \"seniors\", \"old age\", \"ageing\", \"aging\", \"old folks\"],\n",
    "    \"Person with Disabilities or Special Needs\": [\"pwd\", \"disabled\", \"disability\", \"disabilities\", \"handicapped\", \"special needs\", \"syndrome\", \"autism\", \"autistic\", \n",
    "                                                  \"bedridden\", \"bed ridden\", \"bed-ridden\", \"disorder\"],\n",
    "    \"Caregiver\": [\"caregiver\", \"caregiving\", \"caring\", \"look after\", \"daytime support\", \"homebound\", \"bedridden\"],\n",
    "    \"Child or Youth\": [\"teen\", \"teenage\", \"teenager\", \"youth\", \"youths\", \"student\", \"students\", \"orphan\", \"exam\", \"exams\", \"rebellious\", \"tuition\", \"underaged\"],\n",
    "    \"Ex-Offender or Incarcerated Individual\": [\"offender\", \"offenders\", \"prison\", \"jail\", \"released\"],\n",
    "    \"Migrant or Foreign Worker\": [\"migrant\", \"migrants\", \"foreign worker\", \"domestic worker\", \"maid\", \"helper\", \"transnational\"],\n",
    "    \"Woman in Need of Support\": [\"woman\", \"single mother\", \"single mothers\", \"single mum\", \"single mums\", \"young mother\", \"young mothers\", \"young mum\", \"young mums\", \"widow\", \"female\", \"milk\", \n",
    "                                 \"abortion\", \"pregnant\", \"pregnancy\"],\n",
    "    \"Facing Mental Health Challenges\": [\"depression\", \"depressed\", \"depressive\", \"anxious\", \"anxiety\", \"anxieties\", \"mental\", \"therapy\", \"therapist\", \"lonely\", \"no friends\", \"no friend\", \n",
    "                                        \"isolated\", \"suicidal\", \"suicide\"],\n",
    "    \"Homeless or in Need of Shelter\": [\"homeless\", \"no home\", \"shelter\", \"rough sleeping\", \"evicted\"],\n",
    "    \"Dealing with Addictions or Recovery\": [\"alcohol\", \"drinking\", \"gambling\", \"addiction\", \"addictions\", \"rehab\", \"rehabilitation\", \"drugs\", \"vaping\", \"vape\", \"kpod\", \"sex\", \"porn\", \n",
    "                                            \"pornography\", \"gaming\", \"games\"],\n",
    "    \"Facing End-of-Life or Terminal Illness\": [\"end of life\", \"terminal\", \"terminally ill\", \"palliative\", \"final stage\", \"advanced stage\", \"cancer\", \"dying\"],\n",
    "    \"In Need of Legal Aid\": [\"legal\", \"lawyer\", \"court\"],\n",
    "    \"Experiencing Abuse or Violence\": [\"abuse\", \"abused\", \"abusive\", \"violence\", \"violent\", \"sexual\", \"rape\", \"bullying\", \"bullied\", \"bullies\", \"bully\", \"harassment\", \"harassed\"],\n",
    "}\n",
    "\n",
    "# --- Classification Function ---\n",
    "def classify_query(query: str):\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    matched_schemes = [\n",
    "        scheme for scheme, keywords in SCHEME_CATEGORIES.items()\n",
    "        if any(re.search(rf\"\\b{kw}\\b\", query_lower) for kw in keywords)\n",
    "    ]\n",
    "    \n",
    "    matched_profiles = [\n",
    "        profile for profile, keywords in USER_PROFILES.items()\n",
    "        if any(re.search(rf\"\\b{kw}\\b\", query_lower) for kw in keywords)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"schemes\": matched_schemes or [\"Unclassified\"],\n",
    "        \"profiles\": matched_profiles or [\"Unclassified\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2119ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile[\"classification\"] = df_profile[\"who_am_i\"].apply(classify_query)\n",
    "df_profile[\"profile_category\"] = df_profile[\"classification\"].apply(lambda x: x[\"profiles\"])\n",
    "df_profile = df_profile.drop(columns=[\"classification\"])\n",
    "\n",
    "df_profile[\"classification\"] = df_profile[\"looking_for\"].apply(classify_query)\n",
    "df_profile[\"scheme_category\"] = df_profile[\"classification\"].apply(lambda x: x[\"schemes\"])\n",
    "df_profile = df_profile.drop(columns=[\"classification\"])\n",
    "\n",
    "df_profile_filtered = df_profile[['query_text', 'query_timestamp', 'scheme_category', 'profile_category']] \\\n",
    "[~((df_profile['scheme_category'].apply(lambda x: \"Unclassified\" in x)) | (df_profile['profile_category'].apply(lambda x: \"Unclassified\" in x)))] \\\n",
    ".sort_values(by='query_timestamp')\n",
    "\n",
    "df_profile.to_csv('profile_classification.csv', index=False)\n",
    "\n",
    "# df_profile2 = df_profile[['query_text', 'query_timestamp', 'scheme_category', 'profile_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64414bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile[df_profile[\"scheme_category\"].apply(lambda x: \"Unclassified\" in x) & df_profile[\"profile_category\"].apply(lambda x: \"Unclassified\" in x)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0affc50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_untag[\"classification\"] = df_untag[\"query_text\"].apply(classify_query)\n",
    "\n",
    "df_untag[\"scheme_category\"] = df_untag[\"classification\"].apply(lambda x: x[\"schemes\"])\n",
    "df_untag[\"profile_category\"] = df_untag[\"classification\"].apply(lambda x: x[\"profiles\"])\n",
    "\n",
    "df_untag = df_untag.drop(columns=[\"classification\"])\n",
    "\n",
    "df_untag_filtered = df_untag[['query_text', 'query_timestamp', 'scheme_category', 'profile_category']] \\\n",
    "[~((df_untag['scheme_category'].apply(lambda x: \"Unclassified\" in x)) | (df_untag['profile_category'].apply(lambda x: \"Unclassified\" in x)))] \\\n",
    ".sort_values(by='query_timestamp')\n",
    "\n",
    "# df_untag2 = df_untag[['query_text', 'query_timestamp', 'scheme_category', 'profile_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9733662",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.concat([df_profile_filtered, df_untag_filtered], ignore_index=True).sort_values(by='query_timestamp')\n",
    "# df_merged.to_csv('classification.csv', index=False)\n",
    "\n",
    "# df_merged_all = pd.concat([df_profile2, df_untag2], ignore_index=True)\n",
    "# df_merged_all.to_csv('all_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a1409",
   "metadata": {},
   "source": [
    "Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03af9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = df_merged.explode('profile_category').explode('scheme_category')\n",
    "df_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile Categories\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "profile_counts = df_exploded['profile_category'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = profile_counts.plot(kind='barh', color='skyblue')\n",
    "plt.title('Count of Profile Categories')\n",
    "# plt.xlabel('Count')\n",
    "# plt.ylabel('Profile Category')\n",
    "plt.ylabel('')\n",
    "plt.gca().invert_yaxis()  # Highest value on top\n",
    "\n",
    "for i, v in enumerate(profile_counts):\n",
    "    ax.text(v + 0.5, i, str(v), color='black', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d6660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheme Categories\n",
    "\n",
    "scheme_counts = df_exploded['scheme_category'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = scheme_counts.plot(kind='barh', color='teal')\n",
    "plt.title('Count of Scheme Categories')\n",
    "# plt.xlabel('Count')\n",
    "# plt.ylabel('Scheme Category')\n",
    "plt.ylabel('')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "for i, v in enumerate(scheme_counts):\n",
    "    ax.text(v + 0.5, i, str(v), color='black', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f35b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheme_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d817af",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemes_demand_counts = (\n",
    "    df_exploded['scheme_category']\n",
    "    .value_counts()\n",
    "    .reset_index()\n",
    "    .rename(columns={'scheme_category': 'schemes', 'count': 'demand_count'})\n",
    ")\n",
    "\n",
    "schemes_demand_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba65832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = pd.crosstab(df_exploded['profile_category'], df_exploded['scheme_category'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(cross, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Scheme Category')\n",
    "plt.ylabel('Profile Category')\n",
    "plt.title(\"Profile vs Scheme Co-Occurrence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dae9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "pairs = []\n",
    "for cats in df_merged['profile_category']:\n",
    "    pairs += list(itertools.combinations(sorted(cats), 2))\n",
    "\n",
    "pd.Series(pairs).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "pairs = []\n",
    "for cats in df_merged['scheme_category']:\n",
    "    pairs += list(itertools.combinations(sorted(cats), 2))\n",
    "\n",
    "pd.Series(pairs).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c665f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "pairs_schemes = []\n",
    "\n",
    "for _, row in df_merged.iterrows():\n",
    "    profiles = row['profile_category']\n",
    "    schemes = row['scheme_category']\n",
    "\n",
    "    if isinstance(profiles, list) and isinstance(schemes, list):\n",
    "        for pair in itertools.combinations(sorted(profiles), 2):\n",
    "            for scheme in schemes:\n",
    "                pairs_schemes.append({'profile_pair': pair, 'scheme': scheme})\n",
    "\n",
    "df_pairs = pd.DataFrame(pairs_schemes)\n",
    "\n",
    "# Count how often each (profile_pair, scheme) combination appears\n",
    "pair_scheme_counts = (\n",
    "    df_pairs.groupby(['profile_pair', 'scheme'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .sort_values('count', ascending=False)\n",
    ")\n",
    "\n",
    "pair_scheme_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ae8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# with open('filtered_chats.csv', 'w', newline='') as csvfile:\n",
    "#     # Create a csv.writer object\n",
    "#     writer = csv.writer(csvfile)\n",
    "\n",
    "#     # Write all rows at once using writerows()\n",
    "#     writer.writerows(filtered_words)\n",
    "\n",
    "# filtered_words = pd.DataFrame(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8153a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schemes = pd.read_csv('prod_snapshot/firestore-schemes-prod.csv')\n",
    "df_schemes.head()\n",
    "# df_schemes.shape\n",
    "# df_schemes['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6599b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schemes[\"classification\"] = df_schemes[\"scheme_type\"].fillna(\"\").apply(classify_query)\n",
    "df_schemes[\"scheme_category\"] = df_schemes[\"classification\"].apply(lambda x: x[\"schemes\"])\n",
    "df_schemes = df_schemes.drop(columns=[\"classification\"])\n",
    "\n",
    "df_schemes.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befac503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_schemes[['id', 'description']][df_schemes['scheme_category'].apply(lambda x: \"Unclassified\" in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc577227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual editing to populate missing / unclassified categories\n",
    "\n",
    "# df_schemes.loc[df_schemes['id'] == '5aiAf1OrsPxleUJt1dpp', 'scheme_category'] = [\"Family Support Services\"]\n",
    "# df_schemes.loc[df_schemes['id'] == 'IJJDUnxemNNssSAU79Ub', 'scheme_category'] = [\"Family Support Services\"]\n",
    "# df_schemes.loc[df_schemes['id'] == 'NxFf3X2H8LhLMu5UgEBq', 'scheme_category'] = [\"Financial Assistance Programs\"]\n",
    "# df_schemes.loc[df_schemes['id'] == 'PYkEuLhFx3lrLsoNpnDP', 'scheme_category'] = [\"Financial Assistance Programs\"]\n",
    "# df_schemes.loc[df_schemes['id'] == 'aYd97TpximYYxFv059BC', 'scheme_category'] = [\"Social Work & Casework\"]\n",
    "# df_schemes.loc[df_schemes['id'] == 'arLWZDYgtpuPEkc1oABi', 'scheme_category'] = [\"Social Work & Casework\"]\n",
    "# df_schemes.loc[df_schemes['id'] == 'jiEIP4b8CXcXGddC1kBc', 'scheme_category'] = [\"Family Support Services\"]\n",
    "# df_schemes.loc[df_schemes['id'] == 'uRviEIAFnGEg0FerpwvN', 'scheme_category'] = [\"Financial Assistance Programs\"]\n",
    "# df_schemes.loc[df_schemes['id'] == 'wYfMHWP9yDXTdh5iYh8u', 'scheme_category'] = [\"Financial Assistance Programs\"]\n",
    "\n",
    "# find the index of the row\n",
    "idx = df_schemes.index[df_schemes['id'] == '5aiAf1OrsPxleUJt1dpp'][0]\n",
    "# assign the list\n",
    "df_schemes.at[idx, 'scheme_category'] = [\"Family Support Services\"]\n",
    "\n",
    "idx = df_schemes.index[df_schemes['id'] == 'IJJDUnxemNNssSAU79Ub'][0]\n",
    "df_schemes.at[idx, 'scheme_category'] = [\"Family Support Services\"]\n",
    "\n",
    "idx = df_schemes.index[df_schemes['id'] == 'NxFf3X2H8LhLMu5UgEBq'][0]\n",
    "df_schemes.at[idx, 'scheme_category'] = [\"Financial Assistance Programs\"]\n",
    "\n",
    "idx = df_schemes.index[df_schemes['id'] == 'PYkEuLhFx3lrLsoNpnDP'][0]\n",
    "df_schemes.at[idx, 'scheme_category'] = [\"Financial Assistance Programs\"]\n",
    "\n",
    "idx = df_schemes.index[df_schemes['id'] == 'aYd97TpximYYxFv059BC'][0]\n",
    "df_schemes.at[idx, 'scheme_category'] = [\"Social Work & Casework\"]\n",
    "\n",
    "idx = df_schemes.index[df_schemes['id'] == 'arLWZDYgtpuPEkc1oABi'][0]\n",
    "df_schemes.at[idx, 'scheme_category'] = [\"Social Work & Casework\"]\n",
    "\n",
    "idx = df_schemes.index[df_schemes['id'] == 'jiEIP4b8CXcXGddC1kBc'][0]\n",
    "df_schemes.at[idx, 'scheme_category'] = [\"Family Support Services\"]\n",
    "\n",
    "idx = df_schemes.index[df_schemes['id'] == 'uRviEIAFnGEg0FerpwvN'][0]\n",
    "df_schemes.at[idx, 'scheme_category'] = [\"Financial Assistance Programs\"]\n",
    "\n",
    "idx = df_schemes.index[df_schemes['id'] == 'wYfMHWP9yDXTdh5iYh8u'][0]\n",
    "df_schemes.at[idx, 'scheme_category'] = [\"Financial Assistance Programs\"]\n",
    "\n",
    "# df_schemes.to_csv('scheme_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aaf1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded_schemes = df_schemes[['description', 'scheme_category']].explode('scheme_category')\n",
    "df_exploded_schemes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17098f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemes_supply_counts = (\n",
    "    df_exploded_schemes['scheme_category']\n",
    "    .value_counts()\n",
    "    .reset_index()\n",
    "    .rename(columns={'scheme_category': 'schemes', 'count': 'supply_count'})\n",
    ")\n",
    "\n",
    "schemes_supply_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a24d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute share (%) of total\n",
    "schemes_demand_counts['demand_share'] = schemes_demand_counts['demand_count'] / schemes_demand_counts['demand_count'].sum()\n",
    "schemes_supply_counts['supply_share'] = schemes_supply_counts['supply_count'] / schemes_supply_counts['supply_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18802064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and compute relative gap\n",
    "df_prop_gap = pd.merge(schemes_demand_counts, schemes_supply_counts, on='schemes', how='outer').fillna(0)\n",
    "df_prop_gap['share_gap'] = df_prop_gap['demand_share'] - df_prop_gap['supply_share']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6543a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_prop_gap_sorted = df_prop_gap.sort_values('share_gap', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(df_prop_gap_sorted['schemes'], df_prop_gap_sorted['share_gap'] * 100, color='skyblue')\n",
    "plt.axvline(0, color='black', linewidth=0.8)\n",
    "plt.title('Proportional Gap: User Demand vs Available Schemes')\n",
    "plt.xlabel('Difference Between Demand Share & Supply Share (%)')\n",
    "plt.ylabel('Scheme Category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop_gap_sorted = df_prop_gap.sort_values('demand_share', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(df_prop_gap_sorted['schemes'], df_prop_gap_sorted['demand_share'] * 100, label='Demand', alpha=0.6)\n",
    "plt.barh(df_prop_gap_sorted['schemes'], -df_prop_gap_sorted['supply_share'] * 100, label='Supply', alpha=0.6)\n",
    "plt.axvline(0, color='black', linewidth=0.8)\n",
    "plt.title('Proportional Demand vs Supply by Scheme Category')\n",
    "plt.xlabel('Share of Total (%)')\n",
    "plt.xticks(\n",
    "    [-20, -10, 0, 10, 20],\n",
    "    ['20% Supply', '10% Supply', '0', '10% Demand', '20% Demand']\n",
    ")\n",
    "# plt.text(-max(df_prop_gap['supply_share'])*100*1.1, -1, \"Supply\", ha='right', fontsize=10, color='gray')\n",
    "# plt.text(max(df_prop_gap['demand_share'])*100*1.1, -1, \"Demand\", ha='left', fontsize=10, color='gray')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0299899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demand_by_profile = (\n",
    "#     df_exploded.groupby(['profile_category', 'scheme_category'])\n",
    "#     .size()\n",
    "#     .groupby(level=0)\n",
    "#     .apply(lambda x: x / x.sum())  # convert to within-profile proportion\n",
    "#     .reset_index(name='demand_share')\n",
    "# )\n",
    "\n",
    "demand_by_profile = (\n",
    "    df_exploded.groupby(['profile_category', 'scheme_category'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "\n",
    "# Compute within-profile proportions\n",
    "demand_by_profile['demand_share'] = (\n",
    "    demand_by_profile['count'] /\n",
    "    demand_by_profile.groupby('profile_category')['count'].transform('sum')\n",
    ")\n",
    "\n",
    "demand_by_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Pivot for plotting\n",
    "pivot_df = demand_by_profile.pivot(\n",
    "    index='profile_category',\n",
    "    columns='scheme_category',\n",
    "    values='demand_share'\n",
    ").fillna(0)\n",
    "\n",
    "# Plot\n",
    "pivot_df.plot(\n",
    "    kind='barh',\n",
    "    stacked=True,\n",
    "    figsize=(10, 6),\n",
    "    width=0.8,\n",
    "    colormap='tab20'\n",
    ")\n",
    "\n",
    "plt.title(\"Share of Demand by Scheme Category for Each Profile\", fontsize=14)\n",
    "plt.xlabel(\"Proportion of Demand\")\n",
    "plt.ylabel(\"Profile Category\")\n",
    "plt.legend(title=\"Scheme Category\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schemessg-data-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
